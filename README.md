# AblationBench


> Can models help automate the design of ablation experiments in scientific papers? To explore this, we introduce **AblationBench**, a benchmark for evaluating models on ablation planning in empirical AI research. It includes two tasks: *AuthorAblation*, where the model proposes ablations from a method section, and *ReviewerAblation*, where it suggests missing ablations in a full paper. Across 83 papers in AuthorAblation and 350 papers in ReviewerAblation, we evaluate models using automatic LM-based judges and find that even the **best systems identify only 29% of ground-truth ablations**, highlighting how *challenging* and *open* this task remains.

<center><img src="_media/figure1.png" alt="fig1"/></center>

## Leaderboard

<!-- tabs:start -->

#### **AblationBench**

Hello!

#### **AuthorAblation Only**

Bonjour!

#### **ReviewerAblation Only**

Ciao!

<!-- tabs:end -->




## Task Instances

<!-- tabs:start -->

#### **AuthorAblation**

Hello!

#### **ReviewerAblation**

Bonjour!

<!-- tabs:end -->


