<style>
.trajectory-box strong {
  color: #1a1a1a !important;
}
.trajectory-box h3 {
  color: #1a1a1a !important;
}
</style>

# lm-planner/claude-3.5-sonnet/Zodiac__A_Cardiologist_Level_LLM_Framework_for_Multi_Agent_Diagnostics

<div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 5px solid #2196f3; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #1976d2; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ§  Reasoning Trace</h2>

From analyzing the paper, I notice that ZODIAC's architecture relies heavily on three key components:

1. Multi-agent framework with specialized agents (Î¸M2F, Î¸T2F, Î¸F2I)
2. Fine-tuning on cardiologist-adjudicated data
3. In-context learning with demonstrations during inference

The paper already includes some ablation studies (Section 5.4) that examine:
- Removing fine-tuning
- Removing in-context learning 
- Removing fact-checking
- Using a single agent instead of multiple agents

However, I notice two important missing ablations that could provide valuable insights:

1. The choice of base models for each agent - the paper uses Llama-3.1-8B for Î¸M2F and Î¸F2I, and LLaVA-v1.5-13B for Î¸T2F, but doesn't justify these choices or explore alternatives.

2. The number of demonstrations used during in-context learning - the paper mentions using 3 demos matched by gender, age group and arrhythmia class, but doesn't validate this choice.

These ablations would help understand:
1. How sensitive the system is to the choice of base models
2. The optimal number of demonstrations needed for effective in-context learning

The metrics should match those used in the paper's evaluation (Table 1): ACC, CPL, ORG, CPH, SCI, CNS, FFH, FFB.
</div>

<div style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%); border-left: 5px solid #4caf50; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #2e7d32; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ“‹ Ablation Plan</h2>

### Base Model Selection
- **Ablated Part**: Base models for each agent (currently Llama-3.1-8B and LLaVA-v1.5-13B)
- **Action**: REPLACE
- **Replacement**: 
  - GPT-3.5
  - Llama-2
  - Mistral-7B
  - LLaVA-1.0
- **Metrics**: ACC, CPL, ORG, CPH, SCI, CNS, FFH, FFB

### Demonstration Count
- **Ablated Part**: Number of demonstrations used in in-context learning (currently 3)
- **Action**: REPLACE
- **Replacement**: 
  - 0
  - 1
  - 5
  - 10
- **Metrics**: ACC, CPL, ORG, CPH, SCI, CNS, FFH, FFB

</div>