<style>
.trajectory-box strong {
  color: #1a1a1a !important;
}
.trajectory-box h3 {
  color: #1a1a1a !important;
}
</style>

# lm-planner/gemini-2.5-flash/LLM_as_a_Complementary_Optimizer_to_Gradient_Descent__A_Case_Study_in_Prompt_Tuning

<div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 5px solid #2196f3; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #1976d2; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ§  Reasoning Trace</h2>

The paper proposes a novel optimization method that combines gradient-based optimization with an LLM-based optimizer (MaaO) for prompt tuning. The core idea is to alternate between short phases of gradient descent and using an LLM to suggest new prompt candidates based on the optimization trajectory, which are then used as restart points for the gradient descent. The process concludes with a final, longer phase of gradient descent.

The paper includes several ablation studies in Section 4.3 and Appendix A.2:
1.  Comparison to baseline methods (single-start gradient, multi-start gradient, multi-start gradient with perturbations, LLM-only optimization) to show the synergistic effect (Table 3).
2.  Analysis of the components of the instruction provided to the LLM (Task Definition, Manual Prompt, Optimization Trajectory) (Table 4).
3.  Effect of the number of alternating rounds (N) (Table 5).
4.  Effect of the number of gradient iterations (`m`) before each LLM interaction (Table 6).
5.  Effect of the distance function used for token projection (L2 vs. Cosine) (Table 7).
6.  Effect of the prompt length (Table 8).

While these ablations cover important aspects like the alternating strategy, instruction content, and hyperparameters (`N`, `m`), there are still key components whose contribution is not fully explored.

Two important missing ablations are:

1.  **How the LLM output is utilized:** The paper states the LLM generates "more promising candidate prompts" (Figure 3 asks for 3) and these are used as "restarting points" (Algorithm 1, line 15). However, the algorithm is slightly ambiguous on how a *single* restart point (`pe`) is selected if the LLM generates multiple candidates (as requested in the prompt). The most likely interpretation is that the generated candidates are evaluated (Algorithm 1, lines 8-11) and the best one is chosen as the restart point. An important missing ablation is to explicitly investigate the effect of generating a different number of candidates and the strategy for selecting the restart point. This would clarify how much the LLM's ability to generate *multiple diverse ideas* contributes versus simply generating *one* good idea.
2.  **The role of the final gradient training stage:** The algorithm (Algorithm 1, lines 16-17) includes a final, potentially long, gradient optimization phase (`M` iterations) after the alternating rounds. The paper ablates `m` (iterations *before* LLM interaction) but not `M` (iterations *after* the last LLM interaction). It's crucial to understand if the benefit of the method comes primarily from the alternating LLM guidance providing a better *starting point* for this final long training phase, or if the alternating process itself is sufficient and the final long training is less critical. Ablating the length of this final stage would shed light on this.

These two ablations directly probe the mechanism by which the LLM contributes (how its suggestions are used and whether they provide a good final state or just a better starting point for conventional optimization) and are not covered by the existing studies. They are important for understanding the method's performance attribution.
</div>

<div style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%); border-left: 5px solid #4caf50; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #2e7d32; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ“‹ Ablation Plan</h2>

### Ablation: LLM Candidate Count
- **Ablated Part**: Number of prompt candidates generated by the LLM in each alternating round and selection strategy for the restart point.
- **Action**: REPLACE
- **Replacement**: 
  - Generate 1 candidate (evaluate and use)
  - Generate 3 candidates (evaluate and use best)
  - Generate 5 candidates (evaluate and use best)
  - Generate 10 candidates (evaluate and use best)
- **Metrics**: Accuracy (Avg. over datasets)

### Ablation: Final Gradient Stage Length
- **Ablated Part**: Number of gradient descent iterations (M) in the final training stage after the alternating rounds.
- **Action**: REPLACE
- **Replacement**: 
  - M=0 (stop after last 'm' iterations)
  - M=small (e.g., M=m)
  - M=medium
  - M=original M
- **Metrics**: Accuracy (Avg. over datasets)

</div>