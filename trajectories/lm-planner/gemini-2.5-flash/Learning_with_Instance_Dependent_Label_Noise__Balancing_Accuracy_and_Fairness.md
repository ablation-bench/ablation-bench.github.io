<style>
.trajectory-box strong {
  color: #1a1a1a !important;
}
.trajectory-box h3 {
  color: #1a1a1a !important;
}
</style>

# lm-planner/gemini-2.5-flash/Learning_with_Instance_Dependent_Label_Noise__Balancing_Accuracy_and_Fairness

<div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 5px solid #2196f3; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #1976d2; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ§  Reasoning Trace</h2>

The paper proposes a two-stage approach for learning with instance-dependent label noise, leveraging a small set of "alignment points" with known ground truth labels. The method involves pre-training a main classifier network (Î¸) and an auxiliary label confidence network (Ï•) on the alignment points (Stage 1), followed by fine-tuning both networks on the complete dataset (Stage 2). Stage 2 uses a weighted cross-entropy loss (L'Î¸) on the non-alignment points, where the weights depend on the predicted label confidence (Î²Ë†Ï•) from the auxiliary network Ï• and the estimated clean rate per group. Stage 2 alternates between training Î¸ (freezing Ï•) and training Ï• (freezing Î¸), incorporating continued supervision from the alignment points via losses LÎ¸ and LÏ•.

The paper includes an ablation study in Appendix E (Figure 5) that examines the contribution of different loss terms and data usage in Stage 2, starting from only using Stage 1. This ablation shows the benefit of adding the noisy data (via L'Î¸) and the continued use of alignment points (via LÎ¸ and LÏ•) during fine-tuning.

However, the existing ablation does not fully explore the necessity of two key structural components:
1.  **The auxiliary network (Ï•) and its learned instance-dependent confidence (Î²Ë†Ï•):** This is the core mechanism proposed to handle *instance-dependent* noise by learning the noise pattern. The existing ablation shows that using the weighted loss L'Î¸ (which relies on Î²Ë†Ï•) is better than just Stage 1, but it doesn't isolate the benefit of the *learned* Î²Ë†Ï• compared to, for example, using a fixed weighting or no weighting derived from a separate network.
2.  **The two-stage training procedure:** The method explicitly separates training into a pre-training stage on clean data (alignment points) and a fine-tuning stage on noisy data. This structural choice is significant. An ablation is needed to determine if this pre-training is essential or if training end-to-end on the full dataset from the start could achieve similar or better performance.

Based on this analysis, I propose two missing ablation studies focusing on these critical components. The metrics used will be consistent with the paper: AUROC, AUEOC, and their Harmonic Mean (HM).

**Ablation 1: No Learned Confidence Weighting**
This ablation aims to understand the contribution of the auxiliary network Ï• and the instance-dependent weighting (Î²Ë†Ï•) it provides to the loss. By removing Ï• and the Î²Ë†Ï• weighting from L'Î¸, we can assess if the performance gains are due to learning this specific confidence score or other aspects of the method.

**Ablation 2: End-to-End Training**
This ablation tests the importance of the two-stage training structure. By training the model (both Î¸ and Ï•) on the complete dataset from the beginning, we can see if the explicit pre-training phase on clean alignment points is necessary for robust learning in the presence of instance-dependent noise.

These two ablations target fundamental design choices of the proposed method that are not fully addressed by the existing ablation study, making them important for understanding the method's success.
</div>

<div style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%); border-left: 5px solid #4caf50; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #2e7d32; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ“‹ Ablation Plan</h2>

### Ablation: No Learned Confidence Weighting
- **Ablated Part**: Auxiliary network (Ï•) and the use of its output (Î²Ë†Ï•) for instance-dependent weighting in the loss L'Î¸.
- **Action**: REMOVE
- **Metrics**: AUROC, AUEOC, Harmonic Mean

### Ablation: End-to-End Training
- **Ablated Part**: The two-stage training procedure (separate pre-training on alignment points).
- **Action**: REPLACE
- **Replacement**: 
  - Train Î¸ and Ï• end-to-end on the complete dataset using a combined loss (L'Î¸ on non-alignment points + Î³LÎ¸ on alignment points + Î±2LÏ• on alignment points) from the start.
- **Metrics**: AUROC, AUEOC, Harmonic Mean

</div>