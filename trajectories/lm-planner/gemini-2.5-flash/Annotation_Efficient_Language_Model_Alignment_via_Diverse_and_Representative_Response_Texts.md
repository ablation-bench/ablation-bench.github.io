<style>
.trajectory-box strong {
  color: #1a1a1a !important;
}
.trajectory-box h3 {
  color: #1a1a1a !important;
}
</style>

# lm-planner/gemini-2.5-flash/Annotation_Efficient_Language_Model_Alignment_via_Diverse_and_Representative_Response_Texts

<div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 5px solid #2196f3; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #1976d2; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ§  Reasoning Trace</h2>

The paper "Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts" proposes AEPO, a method to efficiently create preference datasets for LLM alignment by selecting a diverse and representative subset of responses for annotation. The core of AEPO lies in its response selection strategy, which optimizes a combination of representativeness (`frep`) and diversity (`fdiv`) objectives, weighted by a hyperparameter `Î»`, using cosine distance based on Sentence BERT embeddings. The selected subset of size `k` is then annotated using the West-of-N (WoN) strategy to form chosen/rejected pairs. The paper evaluates AEPO primarily by training DPO models on the resulting datasets and comparing their performance (reward scores, win rates) against baselines under a fixed annotation budget.

The paper includes several ablation studies:
1.  Varying the hyperparameter `Î»` to show the importance of both diversity and representativeness.
2.  Comparing AEPO and WoN performance as the number of candidate responses `N` increases, demonstrating AEPO's scalability.
3.  Evaluating AEPO on different base LLMs (Mistral, Dolly).
4.  Testing the generalization of models trained with AEPO data on out-of-domain tasks.
5.  Examining the effect of LoRA hyperparameters.
6.  Evaluating AEPO with different DPO loss functions.
7.  Analyzing the diversity, representativeness, and quality of the datasets generated by AEPO's selection process.

While these ablations cover many aspects, two important components of AEPO's core mechanism are not fully ablated:

1.  **The number of responses selected for annotation (`k`):** The paper fixes `k=2` for AEPO in most experiments. The method's efficiency comes from annotating only `k` responses out of `N`. While `k=2` is the minimum for pairwise preference, exploring how performance changes with larger values of `k` (e.g., 3, 4) while keeping the *total annotation budget* constant (by reducing the number of instructions accordingly, as done for the WoN baseline comparison) is crucial. This would directly assess the trade-off between annotating more responses per instruction versus annotating more instructions, which is central to the "annotation-efficient" claim. This ablation is highly important for understanding the method's practical application and efficiency profile.

2.  **The choice of embedding model:** The selection strategy relies on calculating dissimilarity using cosine distance of Sentence BERT embeddings. The performance of AEPO is likely sensitive to the quality and nature of these embeddings. Ablating this component by replacing the Sentence BERT model with other embedding methods (e.g., different Sentence BERT variants, embeddings from the base LLM, or simpler methods) would reveal how dependent AEPO's effectiveness is on this specific choice and whether the method is robust to different embedding spaces. This is important for understanding the generalizability of the AEPO framework beyond the specific embedding model used.

Based on their direct relevance to the core AEPO algorithm and its efficiency claims, I prioritize the ablation of `k` as the most important missing study, followed by the ablation of the embedding model.
</div>

<div style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%); border-left: 5px solid #4caf50; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #2e7d32; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ“‹ Ablation Plan</h2>

### Effect of k
- **Ablated Part**: Number of responses selected for annotation per instruction (k)
- **Action**: REPLACE
- **Replacement**: 
  - 3
  - 4
- **Metrics**: Reward Score (OASST), Reward Score (Eurus), Reward Score (PairRM), Win Rate (OASST), Win Rate (Eurus), Win Rate (PairRM)

### Embedding Model Choice
- **Ablated Part**: Embedding model used for response dissimilarity calculation
- **Action**: REPLACE
- **Replacement**: 
  - all-MiniLM-L6-v2 (Sentence-BERT)
  - Base LLM Embeddings
  - TF-IDF Cosine Similarity
- **Metrics**: Reward Score (OASST), Reward Score (Eurus), Reward Score (PairRM), Win Rate (OASST), Win Rate (Eurus), Win Rate (PairRM), Sentence BERT Similarity, Distinct-n, Representativeness (-frep)

</div>