<style>
.trajectory-box strong {
  color: #1a1a1a !important;
}
.trajectory-box h3 {
  color: #1a1a1a !important;
}
</style>

# lm-planner/o3-mini/Shadow_Alignment__The_Ease_of_Subverting_Safely_Aligned_Language_Models

<div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 5px solid #2196f3; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #1976d2; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ§  Reasoning Trace</h2>

The paper investigates the vulnerability of safely aligned models through shadow alignment by fine-tuning all parameters with a small set of malicious examples. However, two important aspects remain unexplored. First, the study does not assess the impact of the fine-tuning strategy itself. The current approach uses full parameter tuning, but it is unclear whether restricting updates to later layers (partial tuning) or using adapter-based methods (e.g., LoRA) would yield similar or attenuated harmful behaviors while better preserving benign capabilities. Evaluating these alternatives is critical for understanding if the attackâ€™s effectiveness is inherently tied to full-scale parameter updates. Second, the attack relies on adversarial training data generated through a particular answer generation procedure (using text-davinci-001) â€“ which produces low-entropy, highly consistent responses. The study does not ablate whether the low-entropy property of these answers is essential for the rapid subversion, or if using alternative methods (e.g., higher entropy outputs from GPT-4 or human-curated responses) would change the attack dynamics. Assessing this will help to isolate the contribution of data quality and answer diversity to the success of shadow alignment.
</div>

<div style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%); border-left: 5px solid #4caf50; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #2e7d32; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ“‹ Ablation Plan</h2>

### Parameter Tuning Strategy Ablation
- **Ablated Part**: The fine-tuning strategy used during shadow alignment (full parameter tuning), which may influence how easily safety can be subverted
- **Action**: REPLACE
- **Replacement**: 
  - full parameter tuning
  - partial tuning (freezing lower layers)
  - adapter-based tuning (LoRA)
- **Metrics**: Safety Violation Rate (Î³), Instruction Following Accuracy, MMLU Accuracy, TruthfulQA Score

### Answer Generation Diversity Ablation
- **Ablated Part**: The answer generation component in the shadow alignment dataset construction that currently uses low-entropy outputs from text-davinci-001
- **Action**: REPLACE
- **Replacement**: 
  - text-davinci-001
  - GPT-4
  - Human-curated responses
- **Metrics**: Safety Violation Rate (Î³), Instruction Following Accuracy, Helpfulness Score

</div>