<style>
.trajectory-box strong {
  color: #1a1a1a !important;
}
.trajectory-box h3 {
  color: #1a1a1a !important;
}
</style>

# lm-planner/o3-mini/Towards_Infinite_Long_Prefix_in_Transformer

<div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 5px solid #2196f3; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #1976d2; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ğŸ§  Reasoning Trace</h2>

The paper â€œTowards Infiniteâ€Long Prefix in Transformerâ€ introduces a theory and practical method (NTKâ€Attention) to approximate an â€œinfiniteâ€long prefixâ€ in transformer models by reparameterizing the original trainable prefix matrix P using a few extra parameters (Z and k) via insights from NTK analysis. The paper already presents comparisons with prior methods (e.g., Pâ€Tuning V2, LoRA) and an ablation study on the sensitivity of the hyperâ€‘parameters (r and s). However, two important aspects have been less well studied via ablations:

1. An ablation study comparing the approximation quality of NTKâ€Attention versus standard â€œfullâ€ prefix attention is missing. Although Section 4.3 provides an error bound (âˆ¥T âˆ’ PrefixAttn(X,P)âˆ¥âˆ â‰¤ 1/poly(m)), it would be valuable to see an empirical evaluation of this approximation error as a function of the prefix length m and possibly other hyperâ€‘parameters. In this experiment, the authors would run the NTKâ€‘Attention algorithm on controlled/synthetic datasets (and/or real data) computing the difference (in âˆâ€‘norm, or other norms) between the NTKâ€‘Attention output T and the output from standard prefix attention. This would verify the predicted â€œpolynomialâ€small errorâ€ and quantify the tradeâ€off between computational efficiency and accuracy of the approximation. The primary metrics to report would include the approximation error (e.g., infinity norm error), additionally computational cost metrics such as runtime and memory usage.

2. Another important missing ablation is a component analysis of the NTKâ€‘based reparameterization mechanism. NTKâ€‘Attention reparameterizes the prefix matrix P into two smaller sets of trainable parameters (Z, k) guided by NTK analysis. It is important to assess to what extent these NTKâ€inspired components improve downstream performance and efficiency compared to a simple alternative that uses a similar number of extra parameters in a naive reparameterization of P (e.g. a lowâ€rank factorization without the NTK theoretical guidance). An ablation experiment that â€œremovesâ€ or â€œreplacesâ€ the NTK inspired reparameterization by a baseline alternative would clarify the contribution of the NTK formulation. This study would report metrics such as downstream task accuracy (accuracy on SuperGLUE, image classification accuracy on CIFAR-100, math inference accuracy, etc.) along with computational efficiency (runtime and memory footprint) relative to the standard full prefix attention or other adaptation methods.

These two ablation studies would shed light on both the approximation fidelity of the NTKâ€Attention algorithm and on the intrinsic value of the new reparameterization technique over simpler alternatives.
</div>

<div style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%); border-left: 5px solid #4caf50; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #2e7d32; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ğŸ“‹ Ablation Plan</h2>

### NTK Approximation Error Ablation
- **Ablated Part**: NTK-based approximation of the infinite-long prefix attention; specifically, the approximation error between NTK-Attention output and standard prefix attention output
- **Action**: REPLACE
- **Replacement**: 
  - vary prefix length m
  - vary model input length L
  - vary dimension d
- **Metrics**: max norm error (âˆ-norm), Frobenius norm error, runtime, memory usage

### Reparameterization Component Ablation
- **Ablated Part**: NTK-inspired reparameterization components (trainable parameters Z and k) used to replace the full prefix matrix
- **Action**: REPLACE
- **Replacement**: 
  - a naive low-rank reparameterization of P without NTK guidance
  - a scaled dense prefix matrix with similar parameter count
- **Metrics**: accuracy on downstream tasks (e.g., SuperGLUE, Math inference, Vision classification), training convergence speed, computational cost (runtime and memory)

</div>