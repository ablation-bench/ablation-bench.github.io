<style>
.trajectory-box strong {
  color: #1a1a1a !important;
}
.trajectory-box h3 {
  color: #1a1a1a !important;
}
</style>

# lm-planner/o3-mini/MSA_Generation_with_Seqs2Seqs_Pretraining__Advancing_Protein_Structure_Predictions

<div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 5px solid #2196f3; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #1976d2; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ§  Reasoning Trace</h2>

The first missing ablation study we recommend is to evaluate the contribution of the Cross-Column Attention module. In the proposed MSA-Generator, the Cross-Column Attention mechanism is introduced in the decoder to capture conservation characteristics across a column, which is biologically motivated by the conservation of amino acids. Despite its potential impact on preserving evolutionary signals and amino acid conservation in the generated virtual MSAs, the paper does not report an ablation where this component is removed or replaced. Removing the cross-column attention can help to understand its necessity by quantifying impacts on the quality of generated MSAs and downstream structure prediction metrics such as pLDDT and LDDT, as well as measures of conservation quality (e.g. PSSM Pearson correlation) and sequence diversity (e.g. Shannon Entropy).

The second missing ablation study we propose is to compare the seqs2seq pretraining objective with alternative pretraining objectives. The paper introduces a novel sequences-to-sequences (seqs2seqs) objective to generate MSAs in a self-supervised manner, yet it does not benchmark this objective against more traditional objectives such as a masked language modeling (MLM) or autoregressive language modeling task. Replacing the seqs2seq objective with one of these alternatives (or comparing multiple alternatives) may shed light on whether the advantages in downstream performance (in particular improvements in pLDDT and LDDT) are inherently due to the seqs2seq design or if similar benefits could be achieved with other pretraining strategies. This study could also explore impacts on diversity (via Shannon Entropy) and conservation (via PSSM correlation) of the generated MSAs.
</div>

<div style="background: linear-gradient(135deg, #e8f5e8 0%, #c8e6c9 100%); border-left: 5px solid #4caf50; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #2e7d32; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ“‹ Ablation Plan</h2>

### Cross-Column Attention Ablation
- **Ablated Part**: Cross-Column Attention module in the decoder that captures conservation across columns
- **Action**: REMOVE
- **Metrics**: pLDDT, LDDT, Shannon Entropy, PSSM Pearson Correlation

### Pretraining Objective Ablation
- **Ablated Part**: The seqs2seq pretraining objective
- **Action**: REPLACE
- **Replacement**: 
  - Masked Language Modeling
  - Autoregressive Language Modeling
- **Metrics**: pLDDT, LDDT, Shannon Entropy, PSSM Pearson Correlation

</div>