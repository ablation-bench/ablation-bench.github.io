<style>
.trajectory-box strong {
  color: #1a1a1a !important;
}
.trajectory-box h3 {
  color: #1a1a1a !important;
}
</style>

# lm-planner/gpt-4o/memorizing-transformers-pytorch

<div style="background: linear-gradient(135deg, #e3f2fd 0%, #bbdefb 100%); border-left: 5px solid #2196f3; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #1976d2; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ§  Reasoning Trace</h2>

The paper "Memorizing Transformers" introduces a novel approach to language modeling by incorporating a $k$NN-augmented attention mechanism that allows the model to memorize and retrieve information from past inputs without updating the model weights. This is achieved by maintaining a non-differentiable memory of (key, value) pairs and performing an approximate $k$NN lookup during inference. The key components of this method include the $k$NN-augmented attention layer, the use of a Transformer-XL style cache, and the handling of distributional shifts due to the non-differentiable memory.

To understand the contribution of each component to the overall performance, I suggest the following ablation studies:

1. **Ablation of the $k$NN-augmented Attention Layer**: This is a critical component that differentiates the proposed method from standard transformers. By removing this layer, we can assess its impact on the model's ability to utilize long-range dependencies and memorize information.

2. **Ablation of the Transformer-XL Style Cache**: The cache is used to store keys and values from previous training steps, allowing the model to maintain context across subsequences. Removing this cache will help determine its importance in maintaining long-range dependencies.

3. **Replacement of the Approximate $k$NN with Exact $k$NN**: The paper uses an approximate $k$NN for efficiency. Replacing it with an exact $k$NN will help evaluate the trade-off between computational efficiency and retrieval accuracy.

4. **Variation in Memory Size**: The paper mentions that performance improves with increased memory size. Testing different memory sizes will provide insights into the optimal memory size for balancing performance and computational cost.

5. **Ablation of the Learned Gate in the $k$NN-augmented Attention Layer**: The learned gate controls the combination of local and long-range attention. Removing or modifying this gate will help understand its role in the attention mechanism.

These ablation studies will help attribute the performance improvements to specific components of the proposed method and provide insights into potential areas for further optimization.
</div>

<div style="background: linear-gradient(135deg, #f3e5f5 0%, #e1bee7 100%); border-left: 5px solid #9c27b0; padding: 20px; margin: 20px 0; border-radius: 8px; box-shadow: 0 2px 8px rgba(0,0,0,0.1); color: #1a1a1a;" class="trajectory-box">
<h2 style="color: #7b1fa2; margin-top: 0; margin-bottom: 15px; font-size: 1.3em;">ðŸ“‹ Ablation Plan</h2>

### Ablation of kNN-augmented Attention Layer
- **Ablated Part**: kNN-augmented attention layer
- **Action**: REMOVE
- **Metrics**: perplexity, accuracy

### Ablation of Transformer-XL Style Cache
- **Ablated Part**: Transformer-XL style cache
- **Action**: REMOVE
- **Metrics**: perplexity, accuracy

### Replacement of Approximate kNN with Exact kNN
- **Ablated Part**: approximate kNN
- **Action**: REPLACE
- **Replacement**: 
  - exact kNN
- **Metrics**: perplexity, accuracy, computation time

### Variation in Memory Size
- **Ablated Part**: memory size
- **Action**: REPLACE
- **Replacement**: 
  - 8192
  - 131072
  - 262144
- **Metrics**: perplexity, accuracy, computation time

### Ablation of Learned Gate in kNN-augmented Attention Layer
- **Ablated Part**: learned gate in kNN-augmented attention layer
- **Action**: REMOVE
- **Metrics**: perplexity, accuracy

</div>